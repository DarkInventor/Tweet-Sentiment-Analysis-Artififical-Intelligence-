# -*- coding: utf-8 -*-
"""Tweet_SNSCRAPE_Scrapping.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1njn_cHUfVXnS_i9lr4tWjxdfIp43i8R7
"""

# API Key=ArSqGZOxJVnDPhM43YPLNVvvv
# Api key secret = O8EPSpA3LX2XrJLQORlHNxyHDSIk5rpqwCON5pfh3LNVuMDJUd
# Bearer Token = AAAAAAAAAAAAAAAAAAAAAEGMngEAAAAAUWLVqOPXGCeX%2F4qKeWTgegQJGv0%3DdFdQx5zv7w7k2URn48bF52y34VKZUq379MKR6nZjpk8YxuUhP4
# Access_token = 1065218660917264384-7U7cWCDc9lKm3yVtPFqRHwwVPqE1qO
# Acces_token_secret= tIDPBxOtJGzq5xRs0LAk2IJFU4DercGqv9U3r2NyzuEK9
# client_id = d3A4NG5kV0xFblRXU25ZM2M2Ums6MTpjaQ
# client_secret = Wg3ih3xFK4E-L0e7bDu4B-L2BpV-RxDvQOkoW7vCgnSlXh8e31
!pip3 install git+https://github.com/JustAnotherArchivist/snscrape.git

import pandas as pd
from tqdm.notebook import tqdm
import snscrape.modules.twitter as sntwitter

# # Pulling the tweets
# scraper = sntwitter.TwitterSearchScraper("#ArtificialIntelligence")

import subprocess

# Define the hashtag you want to scrape
hashtag = "ArtificialIntelligence"

# Define the maximum number of results
max_results = 100

# Build the snscrape command
command = f"snscrape --max-results {max_results} twitter-hashtag {hashtag}"

# Execute the command and capture the output
output = subprocess.check_output(command, shell=True, text=True)

# Print the output
print(output)

!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json
!snscrape --max-results 100000 --with-entity --jsonl twitter-hashtag ArtificialIntelligence >> output2.json

import pandas as pd
import json

# Read the JSON file into a pandas DataFrame
with open('output2.json') as file:
    data = [json.loads(line) for line in file]
df = pd.DataFrame(data)

# Print the column names
print(df.columns)

df.head()

import pandas as pd
import json

# Read the JSON file into a pandas DataFrame
with open('output2.json') as file:
    data = [json.loads(line) for line in file]
df = pd.DataFrame(data)

# Extract the desired columns
df_processed = df[['date', 'user', 'content', 'id', 'replyCount', 'retweetCount', 'likeCount', 'quoteCount', 'hashtags', 'cashtags', 'mentionedUsers', 'retweetedTweet', 'quotedTweet', 'lang', 'links', 'media']]

# Perform data preprocessing on the 'content' column
df_processed['content'] = df_processed['content'].str.strip()

# Print the processed DataFrame
print(df_processed.head())

df_processed.head()







column_data_types = df_processed.dtypes

print(column_data_types)

df_processed['date'] = pd.to_datetime(df_processed['date'])

df_processed['date'].count()

df_processed.count()

df_processed.head(100)


AI_count = df_processed[df_processed['hashtags'].apply(lambda x: x is not None and 'ArtificialIntelligence' in x)].shape[0]

print("Count of tweets with the hashtag #ArtificialIntelligence:", AI_count)

df_processed.head(100)


AI_count = df_processed[df_processed['content'].apply(lambda x: x is not None and 'ArtificialIntelligence' in x)].shape[0]

print("Count of tweets with the hashtag #ArtificialIntelligence:", AI_count)

df_processed.head(100)
# Filter the DataFrame based on the condition
df_filtered = df_processed[df_processed['hashtags'].apply(lambda x: x is not None and 'ArtificialIntelligence' in x)]

# Print the count of tweets with the hashtag '#ArtificialIntelligence'
print("Count of tweets with the hashtag #ArtificialIntelligence:", df_filtered)

# Continue with further analysis using the filtered DataFrame (df_filtered)

df_filtered.head()

df_filtered.count()

columns_to_drop = ['cashtags', 'mentionedUsers', 'retweetedTweet', 'quotedTweet']
df_filtered = df_filtered.drop(columns_to_drop, axis=1)

df_filtered.head()

df_filtered.count()

df_filtered['links'] = df_filtered['links'].fillna('null')
df_filtered['media'] = df_filtered['media'].fillna('null')
df_filtered.count()

df_filtered.head()

en_tweets = df_filtered[df_filtered['lang'] == 'en']
count_en_tweets = en_tweets.shape[0]
print("Number of English tweets:", count_en_tweets)

qme_tweets = df_filtered[df_filtered['lang'] == 'qme']
count_qme_tweets = qme_tweets.shape[0]
print("Number of qme tweets:", count_qme_tweets)

other_tweets = df_filtered[(df_filtered['lang'] != 'qme') & (df_filtered['lang'] != 'en')]
count_other_tweets = other_tweets.shape[0]
print("Number of tweets in other languages:", count_other_tweets)

other_languages = df_filtered[~df_filtered['lang'].isin(['en', 'qme'])]['lang'].unique()
print("Languages other than 'en' and 'qme':", other_languages)

df_filtered = df_filtered[df_filtered['lang'] == 'en']
df_filtered.count()

df_filtered.head()

df_filtered.to_csv('twitter_English_filtered_scrapper.csv', index=False)

df_filtered = df_filtered[df_filtered['likeCount'] >= 10]

# With 1 Lakh Tweet Scraping got the output of 18 columns with all my filters 
# With 10 Lakh Tweet Scraping got the output of 18 columns with all my filters
# With 1 Cr Tweet Scrapping it got decreased to 16 Columns only with my filters

df_filtered.count()
# df_filtered.head(100)

df_filtered.to_csv('twitter_filtered_scrapper.csv', index=False)

df_filtered.head(100)

# Read the first CSV file
df1 = pd.read_csv('/content/twitter_scrapper.csv')

# Read the second CSV file
df2 = pd.read_csv('/content/twitter_filtered_scrapper.csv')

# Append the second file to the first file
merged_df = pd.concat([df1, df2])

# Save the merged data to a new CSV file
merged_df.to_csv('merged.csv', index=False)

final_data = pd.read_csv("/content/merged.csv")
final_data.head()

final_data.count()

import nltk
nltk.download('vader_lexicon')

#Finding the Compound Score, Positive & negative sentiments from the tweets 
import pandas as pd
from nltk.sentiment import SentimentIntensityAnalyzer

# Read the CSV file into a DataFrame
final_data = pd.read_csv("/content/twitter_English_filtered_scrapper.csv")

# Initialize the sentiment analyzer
analyzer = SentimentIntensityAnalyzer()

# Define a function to get the sentiment scores using VADER
def get_sentiment_scores(text):
    return analyzer.polarity_scores(text)

# Apply sentiment analysis on the 'content' column and create a new column for sentiment scores
final_data['sentiment_scores'] = final_data['content'].apply(get_sentiment_scores)

# Extract the compound sentiment score from the sentiment scores dictionary
final_data['compound_score'] = final_data['sentiment_scores'].apply(lambda scores: scores['compound'])

print(final_data.head())

final_data.count()

#Finding what people are feeling. Count of people who feels optimistic, negative & neural about AI
positive_count = 0
negative_count = 0
neutral_count = 0

for score in final_data['compound_score']:
    if score > 0.2:
        positive_count += 1
    elif score < -0.2:
        negative_count += 1
    else:
        neutral_count += 1

print("Number of people feeling optimistic:", positive_count)
print("Number of people feeling negative:", negative_count)
print("Number of people feeling neutral:", neutral_count)

import matplotlib.pyplot as plt
from wordcloud import WordCloud

# Pie chart
sentiment_counts = [positive_count, negative_count, neutral_count]
sentiment_labels = ['Optimistic', 'Negative', 'Neutral']

plt.pie(sentiment_counts, labels=sentiment_labels, autopct='%1.1f%%')
plt.title('Sentiment Analysis Results')
plt.axis('equal')
plt.show()

# Word cloud
text = ' '.join(final_data['content'])

wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

plt.figure(figsize=(10, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Tweets')
plt.show()

import matplotlib.pyplot as plt

# Bar chart
sentiments = ['Optimistic', 'Negative', 'Neutral']
sentiment_counts = [positive_count, negative_count, neutral_count]

plt.bar(sentiments, sentiment_counts)
plt.title('Sentiment Analysis Results')
plt.xlabel('Sentiment')
plt.ylabel('Count')
plt.show()



